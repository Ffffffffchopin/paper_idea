# 概念： fine-grained Task

fine-grained是类内细分的任务，要求针对更细致的任务进行分类，且标注数据比较昂贵，

所以一般会使用Transfer Learning和Attention的方法进行。

# 论文： Imposing Hard Logical Constraints on Multi-label Classification Neural Networks

用CCN（构建相关网络）将背景知识以普通逻辑的方式作为硬性约束降低了多分类网络在预测时的潜在危险，

使用了分层的约束，一个约束层和一个约束损失函数

# 论文： A Brain-Inspired Hierarchical Reasoning Framework for Cognition-Augmented

# Prosthetic Grasping



提出了一个层次化的框架分层进行感知，概率推理，符号推理，产生复合分布式表征便于分类和推理，使用了高维计算，在假肢抓取实验中解决了预测用户意图的问题。

其中的每一层都是单独训练的,这里的实验数据还不够完善和复杂。

未来可以研究框架的每一册适用于不同用例，或者使用其他概率模型看看是否能映射到高维空间，和其他学习策略的联合使用，或者计算bound以及比较其他框架


# 知乎：# 【薰风读论文】AutoML & NAS综述（一）：自动化的数据准备，特征工程




![[Pasted image 20220902174244.png]]

深度学习模型的选择，建立需要大量时间和经验，因此产生了AutoML,在模型复杂的情况下自动产生模型。

AutoML的pipline中产生模型的部分是NAS（模型网络结构搜索）

![[Pasted image 20220902175211.png]]

### 首先进行数据准备

缺少数据时进行数据合成，数据仿真（openAI gym），生成对抗，

或者在网页上爬取，

用active learning，半监督学习等方法打标签，

有时要fine-tune微调，不平衡时可以重采样或合成新的

### 然后进行特征工程

特征选择是减少冗余的特征，循环迭代评估
![[Pasted image 20220902180150.png]]

特征构建产生新特征

特征提取进行降维

# 知乎 ：看paper用什么方式做笔记最有效率？

论文分为需要精读和泛读的。

泛读的文章先读Abstract和conclusion,再读Intro和experiment，可以高亮（记在纸上）对自己有帮助或者存疑的地方。

记录的时候凝结成一句话，尽量用自己的方式表达。

笔记应该是一种索引，让你记住自己看到过这个方法，不要追求面面俱到，过于贪心。

# 概念：python装饰器

装饰器用来改变函数的属性

# 用法：c++ list

c++ 链表在stl中的实现是list容器，可以前后中间各个位置插入，但遍历需要使用迭代器。

# 知乎： 深度学习中创新点比较小，但是有效果，可以发（水）论文吗?

虽然只改进了一点但深究为什么改变，什么时候改变

用在别的数据集上做应用

深入分析背后的思想，视角变大


做大量实验

